{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor for Kaggle New York City Taxi Fare Prediction [competition](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_summary import DataFrameSummary\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.ensemble import forest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import graphviz\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import re\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data for this competition is 55M rows, each representing a taxi trip in New York City. Our goal is to use the data to provide a fare prediction to the rider based on the pickup time, number of passengers, and the pickup and dropoff locations. \n",
    "\n",
    "**NOTE:** Due to the constraints of the problem statement, we must be careful not to pull in external data that we would not reasonably expect to have access to at the beginning of the taxi ride in training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"./\" # relative path to our data\n",
    "!dir {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100 # load every nth row into df_raw\n",
    "\n",
    "%time df_raw = pd.read_csv(f'{PATH}train.csv', low_memory=False, skiprows=lambda i: i % n != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape # this gives us about half a million rows, plent for quick prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We immidiately see few problems with the data that need to be addressed:\n",
    "1. The minimum fare_amount is -60. We expect it to be a positive number.\n",
    "2. Our latitudes and longitudes for both pickupp and dropoff have some obviously non-sensical values at the extremes.\n",
    "3. I'm reasonably certain a cab can't hold 218 passengers. Let's set a limit of 10.\n",
    "\n",
    "Let's just get rid of these data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Old size: {len(df_raw)}')\n",
    "\n",
    "min_fare = 0\n",
    "max_pass = 10\n",
    "lat_range = [30, 50]\n",
    "lon_range = [-85, -65]\n",
    "\n",
    "df_raw = df_raw[(df_raw.pickup_latitude > lat_range[0]) & \n",
    "                (df_raw.pickup_latitude < lat_range[1]) & \n",
    "                (df_raw.pickup_longitude > lon_range[0]) & \n",
    "                (df_raw.pickup_longitude < lon_range[1]) &\n",
    "                (df_raw.dropoff_latitude > lat_range[0]) & \n",
    "                (df_raw.dropoff_latitude < lat_range[1]) & \n",
    "                (df_raw.dropoff_longitude > lon_range[0]) & \n",
    "                (df_raw.dropoff_longitude < lon_range[1]) & \n",
    "                (df_raw.fare_amount > min_fare) & \n",
    "                (df_raw.passenger_count < max_pass)]\n",
    "print(f'New size: {len(df_raw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took off a little over 10,000 entries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two new features representing the latitude and longitude vectors traversed during the trip\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()\n",
    "\n",
    "add_travel_vector_features(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance – in units of degrees – travelled during each trip\n",
    "def add_distance_feature(df):\n",
    "    df['distance'] = np.sqrt(df.abs_diff_longitude**2 + df.abs_diff_latitude**2)\n",
    "    \n",
    "add_distance_feature(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = df_raw.iloc[:2000].plot.scatter('abs_diff_longitude', 'abs_diff_latitude', alpha=0.5, s=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the distribution for both distance travelled and taxi fare within a limited range. These limits will help us better see the shape of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_rides = df_raw[(df_raw['abs_diff_latitude'] < 0.1) & (df_raw['abs_diff_longitude'] < 0.1)]\n",
    "\n",
    "# Kernel Density Plot for distance travelled \n",
    "fig = plt.figure(figsize=(15,4),)\n",
    "ax=sns.kdeplot(short_rides.distance , color='steelblue',shade=True,label='distance')\n",
    "plt.title('Taxi Ride Distance Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance travelled follows a positive skewed normal distribution, with the max frequency at less than 0.02 degrees (about 1.4 miles). This makes intuitive sense. We'd expect most rides in NYC to be short, likely within Manhattan, with a few longer rides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheap_rides = df_raw[df_raw.fare_amount < 50]\n",
    "\n",
    "# Kernel Density Plot for taxi fare\n",
    "fig = plt.figure(figsize=(15,4),)\n",
    "ax=sns.kdeplot(cheap_rides.fare_amount , color='red',shade=True,label='fare_amount')\n",
    "plt.title('Taxi Ride Fare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, taxi fares follow a very similar distribution. We'd expect the two to be highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation between taxi fare and distance travelled: \"+\n",
    "      f\"{df_raw['fare_amount'].corr(df_raw['distance'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the taxi fare has a high positive correlation with distance travelled. Generally speaking, longer rides are likely to cost more. Shocker!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_raw.drop(['key', 'pickup_datetime', 'fare_amount'], axis=1)\n",
    "y_train = df_raw.fare_amount\n",
    "\n",
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "m.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our quick and dirty Random Forest model has an $R^2$ of 0.96. However, this isn't as impressive a feat as it might seem. First, let's see how it does on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = pd.read_csv(f'{PATH}test.csv')\n",
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run into a problem. Our test data set doesn't actually contain the fare amounts and so we'd have to do one of two things:\n",
    "1. Test our model by submitting to Kaggle. This will give us an RMSE, not an $R^2$.\n",
    "2. Hold out a small portion of our training set as a validation set, retrain our model, and test it on the validation set.\n",
    "\n",
    "Let's do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vals(df, n): return df[:n].copy(), df[n:].copy()\n",
    "\n",
    "n_valid = 9914  # same as Kaggle's test set size\n",
    "n_trn = len(X_train)-n_valid\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "X_train, X_valid = split_vals(X_train, n_trn)\n",
    "y_train, y_valid = split_vals(y_train, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor()\n",
    "m.fit(X_train, y_train)\n",
    "m.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n",
    "\n",
    "def print_score(m):\n",
    "    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n",
    "                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n",
    "    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this simple, no-brainer Random Forest isn't doing a terrible job on our validation set, we could do better. The drop in $R^2$ from training to validation means that we are seriously overfitting our data. Also, if you think about it, an RMSE of \\$4.31 on a taxi fare prediction is rather high!\n",
    "\n",
    "To reiterate, we'd like to be able to beat our **baseline RF Regressor $R^2$ of 0.80**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Random Forest already uses a technique called bagging of multiple trees to give us more generalizable results. To understand what this does, let's take a look at what a single tree with no bagging looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a Subset\n",
    "We don't really need all 500k rows at this stage and can afford to use a small subset of this data for prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = raw_train.sample(40000)\n",
    "X_sub = df_sub.drop(['key', 'pickup_datetime', 'fare_amount'], axis=1)\n",
    "y_sub = df_sub.fare_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "%time m.fit(X_sub, y_sub)\n",
    "m.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a 40,000 row sample of our training set and testing it on the same validation set gives only a small drop in $R^2$. Let's rename our subsets so we can use the print_score function on them, which takes X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_sub\n",
    "y_train = y_sub\n",
    "\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=1, max_depth=3, bootstrap=False, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n",
    "    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n",
    "                      special_characters=True, rotate=True, precision=precision)\n",
    "    IPython.display.display(graphviz.Source(re.sub('Tree {',\n",
    "       f'Tree {{ size={size}; ratio={ratio}', s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_tree(m.estimators_[0], X_train, precision=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not a bad $R^2$!\n",
    "\n",
    "Most of our splits so far are on distance, telling us that from the data available, distance appears to be by some margin the most predictive of the features available. Engineering that feature appears to have been a good decision. While the RF would have made an insight along these lines by splitting on locations instead, having the distance feature available reduces the number of splits necessary.\n",
    "\n",
    "To draw more complex insights, if they exist, we would have to do some more feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagged Trees\n",
    "\n",
    "First, let's go back to our baseline model, which is a forest of randomly bagged trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.stack([t.predict(X_valid) for t in m.estimators_])\n",
    "preds[:,0], np.mean(preds[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our 10 trees – `sklearn` calls them etimators – makes its own (not great) prediction. The mean of these predictions, however, is close to much closer to our known fare value of 9.0 than the individual predictions themselves. This averaging of trees is called bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([metrics.r2_score(y_valid, np.mean(preds[:i+1], axis=0)) for i in range(10)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot suggests that increasing the number of trees beyond 8 or 10 won't help us much. Let's take a look for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That last jump from 40 to 100 estimators made almost no difference for the accuracy of our model.\n",
    "\n",
    "**NOTE**: You may see slightly different numbers when you run this. That's a lesson to me not to randomize how I sample my data. While not always the case, you will likely see some small increase in R^2 as you increase `n_estimators`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engineering Datetime Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of our fields is the pickup_datetime. You may have noticed that I removed this from the dataset before passing it to our model. I shall now train the same RF with the pickup_datetime. Then, I'll engineer some features from the information that this field provides us to see if that improves the model even further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_raw.drop(['key', 'fare_amount'], axis=1)\n",
    "y_train = df_raw.fare_amount\n",
    "\n",
    "n_valid = 9914  # same as Kaggle's test set size\n",
    "n_trn = len(X_train)-n_valid\n",
    "raw_train, raw_valid = split_vals(df_raw, n_trn)\n",
    "X_train, X_valid = split_vals(X_train, n_trn)\n",
    "y_train, y_valid = split_vals(y_train, n_trn)\n",
    "\n",
    "# We should now have 9 cols instead of our earlier 8.\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = raw_train.sample(40000)\n",
    "X_train = raw_train.drop(['key', 'fare_amount'], axis=1)\n",
    "y_train = raw_train.fare_amount\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realize now that, in fact, the Random Forest cannot even use the `pickup_datetime` field in this form since it is a string. This field is of no use to us without some feature engineering. Let's do that and then remove the pickup_datetime field from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datetime_features(df):\n",
    "    \n",
    "    year = lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S %Z\" ).year\n",
    "    df['year'] = df['pickup_datetime'].map(year)\n",
    "    print('1/7')\n",
    "    \n",
    "    hour = lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S %Z\" ).hour\n",
    "    df['hour'] = df['pickup_datetime'].map(hour)\n",
    "    print('2/7')\n",
    "\n",
    "    day_of_week = lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S %Z\" ).weekday()\n",
    "    df['day_of_week'] = df['pickup_datetime'].map(day_of_week)\n",
    "    print('3/7')\n",
    "\n",
    "    month = lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S %Z\" ).month\n",
    "    df['month'] = df['pickup_datetime'].map(month)\n",
    "    print('4/7')\n",
    "\n",
    "    week_number = lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S %Z\" ).strftime('%V')\n",
    "    df['week_number'] = df['pickup_datetime'].map(week_number)\n",
    "    print('5/7')\n",
    "    \n",
    "    seasons = [0,0,1,1,1,2,2,2,3,3,3,0] #dec - feb is winter, then spring, summer, fall etc\n",
    "    season = lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S %Z\" ).month-1)]\n",
    "    df['season'] = df['pickup_datetime'].map(season)\n",
    "    print('6/7')\n",
    "    \n",
    "    # 10pm-5am is late-night\n",
    "    late_night_hours = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "    late_night = lambda x: late_night_hours[(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S %Z\" ).hour)]\n",
    "    df['late_night'] = df['pickup_datetime'].map(late_night)\n",
    "    print('7/7. All done!')\n",
    "    \n",
    "add_datetime_features(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_raw.drop(['key', 'fare_amount', 'pickup_datetime'], axis=1)\n",
    "y_train = df_raw.fare_amount\n",
    "\n",
    "X_train, X_valid = split_vals(X_train, n_trn)\n",
    "y_train, y_valid = split_vals(y_train, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df_raw.sample(40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_sub.drop(['key', 'pickup_datetime', 'fare_amount'], axis=1)\n",
    "y_train = df_sub.fare_amount\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're doing noticeably better in terms of validation R^2, our biggest problem is still that we're overfitting drastically on the training data. The model simply isn't generalizing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I borrowed a function called `set_rf_samples` from the fastai library to help combat this. Instead of sampling our training data up-front like scikit-learn does and then averaging together several `estimators` trained on that data, we could choose a different random subsample for each tree. This way, they don't have the opportunity to fit the whole training set as closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_rf_samples(n):\n",
    "    \"\"\" Changes Scikit learn's random forests to give each tree a random sample of\n",
    "    n random rows.\n",
    "    \"\"\"\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_rf_samples():\n",
    "    \"\"\" Undoes the changes produced by set_rf_samples.\n",
    "    \"\"\"\n",
    "    forest._generate_sample_indices = (lambda rs, n_samples:\n",
    "        forest.check_random_state(rs).randint(0, n_samples, n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have to return to our full dataset to see this technique in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_raw.drop(['key', 'fare_amount', 'pickup_datetime'], axis=1)\n",
    "y_train = df_raw.fare_amount\n",
    "\n",
    "X_train, X_valid = split_vals(X_train, n_trn)\n",
    "y_train, y_valid = split_vals(y_train, n_trn)\n",
    "\n",
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_samples(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that our model performs about the same on the training and validation data now, a sign isn't overfit and that it generalizes well to new observations. What's more, we didn't lose any predictive power in the process and still have an R^2 of 0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the most impressive part is that we did all this without any hyperparameter tuning! A `RandomForestRegressor` with all the defaults (coupled with a few techniques to ensure our model generalizes and some feature engineering) can predict the fare of a taxi using not much more than pickup and dropoff locations and pickup time. It does so with an RMSE of less than 4 dollars, which is not too bad considering our mean fare amount is over 11 dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A real-world model to do something like that is likely to gain a fair bit of predictive accuracy from access to real time traffic flow and routing information. While walking through the hyperparameter tuning process is beyond the scope of this notebook, suffice it to say for now that tweaking some of these defaults can increase performance a little. I will however change the number of subsamples we choose for each tree to see if that helps our accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_rf_samples(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did! This process might seem like a bit of trial and error, and it is. That said, if you're interested, `sklearn.model_selection.RandomizedSearchCV` and `sklearn.model_Selection.GridSearchCV` are useful tools to help automate this process to varying degrees (possibly at the cost of extra computational expense)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = RandomForestRegressor(n_estimators=40, n_jobs=-1)\n",
    "%time m.fit(X_train, y_train)\n",
    "print_score(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained a model with R^2 about 0.05 greater than our baseline and a validation RMSE that's better by just over 60c, let's get predictions on the test data and write them to a submission file that meets Kaaggle's specifications for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = pd.read_csv(f'{PATH}test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw.shape\n",
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_travel_vector_features(test_raw)\n",
    "add_distance_feature(test_raw)\n",
    "add_datetime_features(test_raw)\n",
    "\n",
    "test_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_raw.drop(['pickup_datetime', 'key'], axis=1)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "RF_submission2 = pd.DataFrame(\n",
    "    {'key': test_raw.key, 'fare_amount': y_pred},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "RF_submission2.to_csv('RF_submission.csv', index = False)\n",
    "\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
